import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰æ•°æ®è·¯å¾„å’Œå‘é‡å­˜å‚¨è·¯å¾„
DATA_PATH = "data/"
DB_FAISS_PATH = "vector_store/"


def create_vector_db():
    """
    åˆ›å»ºå‘é‡æ•°æ®åº“å‡½æ•°ï¼ˆæ”¯æŒ ZhipuAI åˆ†æ‰¹åµŒå…¥ï¼‰
    """
    # åŠ è½½ PDF
    loader = DirectoryLoader(DATA_PATH, glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"æˆåŠŸåŠ è½½ {len(documents)} ä»½PDFæ–‡æ¡£ã€‚")

    # from collections import defaultdict
    # page_count = defaultdict(int)
    # for doc in documents:
    #     source = doc.metadata.get("source", "unknown")
    #     page_count[source] += 1
    #
    # for source, count in page_count.items():
    #     print(f"ğŸ“„ {source}: å…± {count} é¡µ")
    # # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ç¯‡ PDF çš„å‰ 500 å­—
    # if documents:
    #     print("\nã€è°ƒè¯•ã€‘ç¬¬ä¸€ç¯‡ PDF å†…å®¹ï¼š")
    #     print(documents[0].page_content)
    #     print("\n" + "=" * 60)

    # åˆ†å‰²æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚")

    provider = os.getenv("LLM_PROVIDER", "deepseek").lower()
    print(f"æ­£åœ¨ä½¿ç”¨æä¾›å•†: {provider}")

    if provider == "zhipuai":
        zhipu_api_key = os.getenv("ZHIPUAI_API_KEY")
        if not zhipu_api_key:
            raise ValueError("é”™è¯¯: LLM_PROVIDER è®¾ç½®ä¸º 'zhipuai', ä½†æœªæ‰¾åˆ° ZHIPUAI_API_KEYã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        print("æ­£åœ¨é€šè¿‡ ZhipuAI API è¿æ¥å¹¶åŠ è½½è¯åµŒå…¥æ¨¡å‹...")
        embeddings = ZhipuAIEmbeddings(
            api_key=zhipu_api_key,
            model=os.getenv("ZHIPUAI_EMBEDDING_MODEL", "embedding-2")
        )
        # ===== æ‰‹åŠ¨åˆ†æ‰¹åµŒå…¥ï¼ˆZhipuAI é™åˆ¶æ¯æ‰¹ â‰¤64ï¼‰=====
        batch_size = 64
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = embeddings.embed_documents([doc.page_content for doc in batch])
            all_embeddings.extend(batch_embeddings)
            print(f"å·²åµŒå…¥ {min(i + batch_size, len(texts))} / {len(texts)} ä¸ªæ–‡æœ¬å—")

        # ä½¿ç”¨ FAISS.from_embeddings æ‰‹åŠ¨æ„å»º
        db = FAISS.from_embeddings(
            text_embeddings=zip([doc.page_content for doc in texts], all_embeddings),
            embedding=embeddings,
            metadatas=[doc.metadata for doc in texts]
        )

    else:  # deepseek æˆ–å…¶ä»– OpenAI å…¼å®¹ API
        if not all([os.getenv("DEEPSEEK_API_KEY"), os.getenv("DEEPSEEK_API_BASE")]):
            raise ValueError("é”™è¯¯: LLM_PROVIDER è®¾ç½®ä¸º 'deepseek' (æˆ–æœªè®¾ç½®), ä½†ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
        print("æ­£åœ¨é€šè¿‡ DeepSeek API è¿æ¥å¹¶åŠ è½½è¯åµŒå…¥æ¨¡å‹...")
        embeddings = OpenAIEmbeddings(
            model=os.getenv("DEEPSEEK_EMBEDDING_MODEL", "text-embedding-v2"),
            openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
            openai_api_base=os.getenv("DEEPSEEK_API_BASE"),
            chunk_size=16  # OpenAIEmbeddings æ”¯æŒè‡ªåŠ¨åˆ†æ‰¹
        )
        # OpenAIEmbeddings å·²å†…ç½®åˆ†æ‰¹ï¼Œå¯ç›´æ¥ä½¿ç”¨ from_documents
        db = FAISS.from_documents(texts, embeddings)

    # ä¿å­˜æ•°æ®åº“
    db.save_local(DB_FAISS_PATH)
    print(f"å‘é‡æ•°æ®åº“æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜äº '{DB_FAISS_PATH}'")


if __name__ == "__main__":
    try:
        create_vector_db()
    except ValueError as e:
        print(e)