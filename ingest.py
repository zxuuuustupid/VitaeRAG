import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰æ•°æ®è·¯å¾„å’Œå‘é‡å­˜å‚¨è·¯å¾„
DATA_PATH = "data/"
DB_FAISS_PATH = "vector_store/"


def create_vector_db():
    """
    åˆ›å»ºå‘é‡æ•°æ®åº“å‡½æ•°ï¼ˆæ”¯æŒ ZhipuAI åˆ†æ‰¹åµŒå…¥ï¼‰
    """
    # åŠ è½½ PDF
    loader = DirectoryLoader(DATA_PATH, glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"æˆåŠŸåŠ è½½ {len(documents)} ä»½PDFæ–‡æ¡£ã€‚")

    # from collections import defaultdict
    # page_count = defaultdict(int)
    # for doc in documents:
    #     source = doc.metadata.get("source", "unknown")
    #     page_count[source] += 1
    #
    # for source, count in page_count.items():
    #     print(f"ğŸ“„ {source}: å…± {count} é¡µ")
    # # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ç¯‡ PDF çš„å‰ 500 å­—
    # if documents:
    #     print("\nã€è°ƒè¯•ã€‘ç¬¬ä¸€ç¯‡ PDF å†…å®¹ï¼š")
    #     print(documents[0].page_content)
    #     print("\n" + "=" * 60)

    # åˆ†å‰²æ–‡æœ¬
    # åˆ†å‰²æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚")

    # === æ–°å¢ï¼šè¿‡æ»¤ä½è´¨é‡/æ— æ„ä¹‰çš„æ–‡æœ¬å— ===
    def is_low_quality(text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬å—æ˜¯å¦ä¸ºä½è´¨é‡å†…å®¹ï¼ˆå¦‚å°é¢ã€å£°æ˜ã€å‚è€ƒæ–‡çŒ®ç­‰ï¼‰"""
        text_clean = text.strip()
        if len(text_clean) < 80:  # å¤ªçŸ­çš„å—é€šå¸¸æ— æ„ä¹‰
            return True

        text_lower = text_clean.lower()
        # å¸¸è§å™ªå£°å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        noise_keywords = [
            "e-mail", "email", "corresponding author", "first author",
            "manuscript draft", "accepted manuscript",
            "declaration of interest", "conflict of interest",
            "editorial manager", "produxiion manager", "aries systems",
            "reference", "references", "bibliography",
            "figure", "fig.", "table", "doi:", "http", "www.",
            "cover letter", "submission", "reviewer", "editor",
            "copyright", "all rights reserved", "abstract"  # æ³¨æ„ï¼šæœ‰äº›æ‘˜è¦å¾ˆçŸ­ï¼Œæ…ç”¨
        ]

        # å¦‚æœåŒ…å«å¤šä¸ªå™ªå£°å…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯å™ªå£°é¡µ
        match_count = sum(1 for kw in noise_keywords if kw in text_lower)
        if match_count >= 2:
            return True

        # ç‰¹æ®Šè§„åˆ™ï¼šå¦‚æœåŒ…å«é‚®ç®±ä½†æ–‡æœ¬å¾ˆçŸ­
        if "@" in text_clean and len(text_clean.split()) < 15:
            return True

        return False

    # æ‰§è¡Œè¿‡æ»¤
    original_count = len(texts)
    texts = [doc for doc in texts if not is_low_quality(doc.page_content)]
    print(f"è¿‡æ»¤ä½è´¨é‡æ–‡æœ¬å—åï¼Œå‰©ä½™ {len(texts)} ä¸ªæœ‰æ•ˆæ–‡æœ¬å—ï¼ˆåŸ {original_count} ä¸ªï¼‰ã€‚")



    provider = os.getenv("LLM_PROVIDER", "deepseek").lower()
    print(f"æ­£åœ¨ä½¿ç”¨æä¾›å•†: {provider}")

    if provider == "zhipuai":
        zhipu_api_key = os.getenv("ZHIPUAI_API_KEY")
        if not zhipu_api_key:
            raise ValueError("é”™è¯¯: LLM_PROVIDER è®¾ç½®ä¸º 'zhipuai', ä½†æœªæ‰¾åˆ° ZHIPUAI_API_KEYã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        print("æ­£åœ¨é€šè¿‡ ZhipuAI API è¿æ¥å¹¶åŠ è½½è¯åµŒå…¥æ¨¡å‹...")
        embeddings = ZhipuAIEmbeddings(
            api_key=zhipu_api_key,
            model=os.getenv("ZHIPUAI_EMBEDDING_MODEL", "embedding-2")
        )
        # ===== æ‰‹åŠ¨åˆ†æ‰¹åµŒå…¥ï¼ˆZhipuAI é™åˆ¶æ¯æ‰¹ â‰¤64ï¼‰=====
        batch_size = 64
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = embeddings.embed_documents([doc.page_content for doc in batch])
            all_embeddings.extend(batch_embeddings)
            print(f"å·²åµŒå…¥ {min(i + batch_size, len(texts))} / {len(texts)} ä¸ªæ–‡æœ¬å—")

        # ä½¿ç”¨ FAISS.from_embeddings æ‰‹åŠ¨æ„å»º
        db = FAISS.from_embeddings(
            text_embeddings=zip([doc.page_content for doc in texts], all_embeddings),
            embedding=embeddings,
            metadatas=[doc.metadata for doc in texts]
        )

    else:  # deepseek æˆ–å…¶ä»– OpenAI å…¼å®¹ API
        if not all([os.getenv("DEEPSEEK_API_KEY"), os.getenv("DEEPSEEK_API_BASE")]):
            raise ValueError("é”™è¯¯: LLM_PROVIDER è®¾ç½®ä¸º 'deepseek' (æˆ–æœªè®¾ç½®), ä½†ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
        print("æ­£åœ¨é€šè¿‡ DeepSeek API è¿æ¥å¹¶åŠ è½½è¯åµŒå…¥æ¨¡å‹...")
        embeddings = OpenAIEmbeddings(
            model=os.getenv("DEEPSEEK_EMBEDDING_MODEL", "text-embedding-v2"),
            openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
            openai_api_base=os.getenv("DEEPSEEK_API_BASE"),
            chunk_size=16  # OpenAIEmbeddings æ”¯æŒè‡ªåŠ¨åˆ†æ‰¹
        )
        # OpenAIEmbeddings å·²å†…ç½®åˆ†æ‰¹ï¼Œå¯ç›´æ¥ä½¿ç”¨ from_documents
        db = FAISS.from_documents(texts, embeddings)

    # ä¿å­˜æ•°æ®åº“
    db.save_local(DB_FAISS_PATH)
    print(f"å‘é‡æ•°æ®åº“æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜äº '{DB_FAISS_PATH}'")


if __name__ == "__main__":
    try:
        create_vector_db()
    except ValueError as e:
        print(e)