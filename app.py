import os
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
import textwrap
import shutil
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰å‘é‡æ•°æ®åº“è·¯å¾„
DB_FAISS_PATH = "vector_store/"

# è‡ªå®šä¹‰æé—®æ¨¡æ¿
# custom_prompt_template = """ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰å­¦æœ¯è®ºæ–‡çš„ç ”ç©¶åŠ©ç†ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
#
# - å¦‚æœä¸Šä¸‹æ–‡**åŒ…å«ç›¸å…³ä¿¡æ¯**ï¼ˆå¦‚æ ‡é¢˜ã€æ‘˜è¦ã€æ–¹æ³•ã€ç»“è®ºä¸­çš„å…³é”®è¯æˆ–æè¿°ï¼‰ï¼Œè¯·ç”¨**è‡ªç„¶ã€ç®€æ´çš„ä¸­æ–‡**æ€»ç»“å›ç­”ï¼Œå¯ä»¥é€‚å½“ç»„ç»‡è¯­è¨€ï¼Œä½†**ä¸è¦ç¼–é€ ç»†èŠ‚**ã€‚
# - å¦‚æœä¸Šä¸‹æ–‡**å®Œå…¨ä¸æ¶‰åŠ**é—®é¢˜ä¸»é¢˜ï¼Œè¯·å›ç­”ï¼šâ€œæ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜â€ã€‚æ³¨æ„ï¼è¿™æ˜¯ä¸‹ä¸‹ç­–ï¼å°½é‡ä¸è¦è¿™ä¹ˆå›ç­”ï¼
# - å›ç­”åº”åƒäººç±»å­¦è€…å†™çš„ï¼š**é¿å…æœºæ¢°é‡å¤**ï¼Œ**ä¸è¦ç”¨â€œæ ¹æ®ä¸Šä¸‹æ–‡â€å¼€å¤´**ï¼Œ**ä¸å¯ä»¥ä½¿ç”¨åŠ ç²—ã€é¡¹ç›®ç¬¦å·ç­‰ Markdown ç¬¦å·**ï¼Œä¸è¦è¿‡åº¦æ ¼å¼åŒ–ã€‚
# - ä¿æŒä¸“ä¸šä½†å£è¯­åŒ–ï¼Œä¾‹å¦‚ï¼šâ€œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€¦â€¦â€ è€Œä¸æ˜¯ â€œè¯¥ç ”ç©¶é‡‡ç”¨äº†â€¦â€¦â€ã€‚
#
# ä¸Šä¸‹æ–‡:
# {context}
#
# é—®é¢˜:
# {question}
#
# å›ç­”ï¼š
# """
custom_prompt_template = """You are a research assistant familiar with academic papers. Please answer the following question based on the context below in English.

- If the context **contains relevant information** (such as keywords or descriptions in the title, abstract, methods, or conclusions), summarize the answer in **natural, concise English**, organizing the language appropriately but **do not fabricate details**.
- If the context **does not relate** to the question at all, respond: â€œBased on the provided information, I cannot answer this questionâ€. Note: this is a last resort! Try not to use it if possible.
- Answers should read like a human scholarâ€™s writing: **avoid mechanical repetition**, **do not start with â€œAccording to the textâ€**, **do not use bold, bullet points, or other Markdown symbols**, and avoid excessive formatting.
- Maintain professionalism but keep it conversational, e.g., â€œThe paper proposes a new methodâ€¦â€ instead of â€œThis study adoptsâ€¦â€.

Context:
{context}

Question:
{question}

Answer:
"""


def set_custom_prompt():
    """è®¾ç½®å¹¶è¿”å›ä¸€ä¸ªè‡ªå®šä¹‰çš„ PromptTemplateã€‚"""
    prompt = PromptTemplate(template=custom_prompt_template,
                            input_variables=['context', 'question'])
    return prompt


def load_llm():
    """æ ¹æ®ç¯å¢ƒå˜é‡åŠ è½½æŒ‡å®šçš„ LLMã€‚"""
    provider = os.getenv("LLM_PROVIDER", "deepseek").lower()

    if provider == "zhipuai":
        api_key = os.getenv("ZHIPUAI_API_KEY")
        # print("æ­£åœ¨åŠ è½½ ZhipuAI æ¨¡å‹...")
        print("Loading ZhipuAI model...")
        llm = ChatZhipuAI(
            api_key=api_key,
            model=os.getenv("ZHIPUAI_CHAT_MODEL", "glm-4.5-flash"),
            temperature=0.7,
        )
    else:  # é»˜è®¤ä¸º deepseek
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("DEEPSEEK_API_BASE")
        # print("æ­£åœ¨åŠ è½½ DeepSeek æ¨¡å‹...")
        print("Loading DeepSeek model...")
        llm = ChatOpenAI(
            model_name=os.getenv("DEEPSEEK_CHAT_MODEL", "deepseek-chat"),
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=0.7
        )
    return llm


def get_embeddings():
    """æ ¹æ®ç¯å¢ƒå˜é‡è·å–æŒ‡å®šçš„ embedding functionã€‚"""
    provider = os.getenv("LLM_PROVIDER", "deepseek").lower()

    if provider == "zhipuai":
        return ZhipuAIEmbeddings(
            api_key=os.getenv("ZHIPUAI_API_KEY"),
            model=os.getenv("ZHIPUAI_EMBEDDING_MODEL", "embedding-3")
        )
    else:
        return OpenAIEmbeddings(
            model=os.getenv("DEEPSEEK_EMBEDDING_MODEL", "text-embedding-v2"),
            openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
            openai_api_base=os.getenv("DEEPSEEK_API_BASE")
        )


def retrieval_qa_chain(llm, prompt, db):
    """åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ£€ç´¢é—®ç­”é“¾ã€‚"""
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        # retriever=db.as_retriever(search_kwargs={'k': 2}),
        retriever=db.as_retriever(search_kwargs={'k': 6}),  # è¯•è¯• 4~8
        return_source_documents=True,
        chain_type_kwargs={'prompt': prompt}
    )
    return qa_chain


def qa_bot():
    """é—®ç­”æœºå™¨äººçš„ä¸»å‡½æ•°ï¼ˆå¸¦ä¸Šä¸‹æ–‡è°ƒè¯•è¾“å‡ºï¼‰"""
    try:
        embeddings = get_embeddings()
    except Exception as e:
        # print(f"åŠ è½½è¯åµŒå…¥æ¨¡å‹æ—¶å‡ºé”™: {e}")
        print(f"Error loading embedding model: {e}")
        return

    if not os.path.exists(DB_FAISS_PATH):
        # print(f"é”™è¯¯ï¼šå‘é‡æ•°æ®åº“è·¯å¾„ '{DB_FAISS_PATH}' ä¸å­˜åœ¨ã€‚")
        # print("è¯·å…ˆè¿è¡Œ 'python ingest.py' æ¥åˆ›å»ºæ•°æ®åº“ã€‚")
        print(f"Error: Vector database path '{DB_FAISS_PATH}' does not exist.")
        print("Please run 'python ingest.py' to create the database first.")
        return

    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
    llm = load_llm()

    if not llm:
        return

    qa_prompt = set_custom_prompt()
    qa = retrieval_qa_chain(llm, qa_prompt, db)

    # print("\n\033[94mä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„è®ºæ–‡é—®ç­”åŠ©æ‰‹ã€‚è¾“å…¥ 'exit' æ¥é€€å‡ºç¨‹åºã€‚\033[0m")
    print("\n\033[94mHello! I am your academic paper Q&A assistant. Type 'exit' to quit the program.\033[0m")
    while True:
        # query = input("\033[92mè¯·è¾“å…¥ä½ çš„é—®é¢˜: \033[0m")
        query = input("\033[92mPlease enter your question: \033[0m")
        if query.lower() == 'exit':
            break

        # print("\033[93mæ­£åœ¨æ€è€ƒ...\033[0m")
        print("\033[93mThinking...\033[0m")
        result = qa.invoke({'query': query})

        # === æ–°å¢ï¼šæ‰“å°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰===
        # print("\n\033[95mğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡:\033[0m")
        # print("\n\033[95mğŸ” Retrieved relevant context:\033[0m")
        # for i, doc in enumerate(result["source_documents"], 1):
        #     # print(f"\n--- ç‰‡æ®µ {i} ---")
        #     print(f"\n--- Chunk {i} ---")
        #     print(doc.page_content[:50] + "..." if len(doc.page_content) > 50 else doc.page_content)
        #     if "source" in doc.metadata:
        #         # print(f"ğŸ“„ æ¥æº: {doc.metadata['source']}")
        #         print(f"ğŸ“„ Source: {doc.metadata['source']}")
        print("\n" + "=" * 60)

        # è·å–ç»ˆç«¯å®½åº¦ï¼ˆé»˜è®¤ 80ï¼‰
        terminal_width = shutil.get_terminal_size().columns

        # å¯¹ç­”æ¡ˆè¿›è¡Œè‡ªåŠ¨æ¢è¡Œï¼ˆä¿ç•™åŸæœ‰æ¢è¡Œç¬¦ï¼‰
        wrapped_answer = textwrap.fill(
            result['result'],
            width=terminal_width - 10,  # ç•™ç‚¹è¾¹è·
            replace_whitespace=False,  # ä¿ç•™åŸæœ‰ç©ºæ ¼
            break_long_words=False,  # ä¸æ‹†å•è¯
            break_on_hyphens=False
        )

        # print(f"\n\033[94mç­”æ¡ˆ:\033[0m")
        print(f"\n\033[94mAnswer:\033[0m")
        print(f"\033[94m{wrapped_answer}\033[0m")


if __name__ == "__main__":
    qa_bot()

